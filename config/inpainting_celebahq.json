{
    "name": "inpainting_celebahq", // experiments name
    "gpu_ids": [0], // gpu ids list, default is single 0
    "seed" : -1, // random seed, seed <0 represents randomization not used 
    "finetune_norm": false, // find the parameters to optimize

    "path": { //set every part file path
        "base_dir": "experiments", // base path for all log except resume_state
        "code": "code", // code backup
        "tb_logger": "tb_logger", // path of tensorboard logger
        "results": "results",
        "checkpoint": "checkpoint",
        "resume_state": "/kaggle/input/o64-321/321" 
        // "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration
    },

    "datasets": { // train or test
        "train": { 
            "which_dataset": {  // import designated dataset using arguments 
                "name": ["data.dataset", "InpaintDataset"], // import Dataset() class / function(not recommend) from data.dataset.py (default is [data.dataset.py])
                "args":{ // arguments to initialize dataset
                    "data_root": "/kaggle/working/huface_origin_64/datasets/celebahq/flist/train_64_28000.flist",
                    "data_len": -1,
                    "mask_config": {
                        "mask_mode": "hybrid",
                        "phase": "train"
                    }
                }
            },
            "dataloader":{
                "validation_split": 4, // percent or number
                "args":{ // arguments to initialize train_dataloader
                    "batch_size": 5, // batch size in each gpu
                    "num_workers": 4,
                    "shuffle": true,
                    "pin_memory": true,
                    "drop_last": true
                },
                "val_args":{ // arguments to initialize valid_dataloader, will overwrite the parameters in train_dataloader
                    "batch_size": 2, // batch size in each gpu
                    "num_workers": 4,
                    "shuffle": false,
                    "pin_memory": true,
                    "drop_last": true
                }
            }
        },
        "test": {
            "which_dataset": {
                "name": "InpaintDataset", // import Dataset() class / function(not recommend) from default file
                "args":{
                    "data_root": "/kaggle/working/huface_origin_64/datasets/celebahq/flist/test_64_2000.flist",
                    "mask_config": {
                        "mask_mode": "center",
                        "phase": "test"
                    }
                }
            },
            "dataloader":{
                "args":{
                    "batch_size": 4,
                    "num_workers": 4,
                    "pin_memory": true
                }
            }
        }
    },

    "model": { // networks/metrics/losses/optimizers/lr_schedulers is a list and model is a dict
        "which_model": { // import designated  model(trainer) using arguments
            "name": ["models.model", "Palette"], // import Model() class / function(not recommend) from models.model.py (default is [models.model.py])
            "args": {
                "sample_num": 8, // process of each image
                "task": "inpainting",
                "ema_scheduler": {
                    "ema_start": 1,
                    "ema_iter": 1,
                    "ema_decay": 0.9999
                },
                "optimizers": [
                    { "lr": 5e-5, "weight_decay": 0}
                ]
            }
        }, 
        "which_networks": [ // import designated list of networks using arguments
            {
                "name": ["models.network", "Network"], // import Network() class / function(not recommend) from default file (default is [models/network.py]) 
                "args": { // arguments to initialize network
                    "init_type": "kaiming", // method can be [normal | xavier| xavier_uniform | kaiming | orthogonal], default is kaiming
                    "module_name": "guided_diffusion", // sr3 | guided_diffusion
                    "unet": {
                        "in_channel": 6,
                        "out_channel": 3,
                        "inner_channel": 64,
                        "channel_mults": [
                            1,
                            2,
                            4,
                            8
                        ],
                        "attn_res": [
                            // 32,
                            16
                            // 8
                        ],
                        "num_head_channels": 32,
                        "res_blocks": 2,
                        "dropout": 0.2,
                        "image_size": 64
                    },
                    "beta_schedule": {
                        "train": {
                            "schedule": "linear",
                            "n_timestep": 2000,
                            // "n_timestep": 10, // debug
                            "linear_start": 1e-6,
                            "linear_end": 0.01
                        },
                        "test": {
                            "schedule": "linear",
                            "n_timestep": 1000,
                            "linear_start": 1e-4,
                            "linear_end": 0.09
                        }
                    }
                }
            }
        ],
        "which_losses": [ // import designated list of losses without arguments
            "mse_loss" // import mse_loss() function/class from default file (default is [models/losses.py]), equivalent to { "name": "mse_loss", "args":{}}
        ],
        "which_metrics": [ // import designated list of metrics without arguments
            "mae" // import mae() function/class from default file (default is [models/metrics.py]), equivalent to { "name": "mae", "args":{}}
        ]
    },

    "train": { // arguments for basic training
        "n_epoch": 366, // max epochs, not limited now
        "n_iter": 1e8, // max interations
        "val_epoch": 1, // valdation every specified number of epochs
        "save_checkpoint_epoch": 10,
        "log_iter": 1e3, // log every specified number of iterations
        "tensorboard" : false,  // tensorboardX enable
        "min_val_mae_loss": 0.03637281805276871,
        "min_val_flag": false,
        "train_previous": [0.14171121259800817, 0.027789672578726807, 0.021782605056307948, 0.019008231214445174, 0.01717432176996337, 0.01705713757985126, 0.01631956299413322, 0.01525886429430704, 0.014911388963170232, 0.015312796009932123, 0.014666173716137564, 0.015023428051796864, 0.014228415989146596, 0.014031730404716882, 0.014156775261692359, 0.013596679461026504, 0.014025686048298895, 0.012988203605891122, 0.01354908014273359, 0.013410291934541714, 0.013390143227129102, 0.013094511619571298, 0.013266082957753049, 0.01378767490228019, 0.013023514449733513, 0.013091798636597988, 0.012520347009517246, 0.013182502645144104, 0.012791115157422444, 0.012310485224638125, 0.0126189018205802, 0.012431390231970759, 0.012637104403562891, 0.012934971962694271, 0.012369826651175581, 0.012427216636002148, 0.011961523567636441, 0.012307649117159255, 0.012684616583881743, 0.01245554559044235, 0.012029736655705786, 0.012718673248145195, 0.012281367397479743, 0.012179195398085756, 0.012047356600321607, 0.012119411223466948, 0.01229985302977656, 0.012401257494348346, 0.012151545002816818, 0.01214480952203966, 0.011658414115874512, 0.011730518156733892, 0.012151588771117931, 0.012156171243556472, 0.011993137221831884, 0.012084316435872844, 0.012399906697366514, 0.01196199795464079, 0.01195070420179781, 0.011904221519124864, 0.011787561456869194, 0.011819349459391547, 0.011594868710168236, 0.011769804247472242, 0.01186894406421027, 0.011771509691152586, 0.01183168673000848, 0.011768121016051881, 0.012284577132969978, 0.011834352081090384, 0.011358533112123308, 0.011649697882548276, 0.011864819155048242, 0.011767525784143012, 0.01133485819474492, 0.011764689030907776, 0.011467596309471039, 0.011541657612369067, 0.011576255267405562, 0.011915649379920703, 0.01129377018286261, 0.011412332242008572, 0.011131772906869461, 0.011633726834847657, 0.011224911291142468, 0.011362639869529428, 0.0112518040676582, 0.011294624949263064, 0.011902085279458303, 0.011612061079349784, 0.011670623031367995, 0.011474494256006678, 0.0114116445854912, 0.01128176809122817, 0.0117656768403455, 0.011137642020299819, 0.011459522520187481, 0.011799764998726706, 0.010990398489446614, 0.011326699157412521, 0.010954232167760996, 0.010937221037766018, 0.01095986681051666, 0.011086713490188438, 0.0110344390384003, 0.010697362379458656, 0.011087672714496816, 0.011497145023595754, 0.011275399971266163, 0.011155475330645856, 0.01132020553029248, 0.011419116065435893, 0.01129828296966807, 0.011290505518817093, 0.01056608651005389, 0.010943340748929492, 0.011280067725250295, 0.010877035972856443, 0.011454823821332815, 0.011064317111858905, 0.010941431840763315, 0.011448721009338967, 0.010812336656552651, 0.011427562269436474, 0.01102140988747911, 0.011424617466897831, 0.011236321784927387, 0.011575541654839358, 0.011402305310053821, 0.011312344884422931, 0.011384055373179913, 0.01099019241918307, 0.011107801243489479, 0.011016574689682881, 0.010810612356481479, 0.01174663474035872, 0.010851115647801444, 0.011337856123824382, 0.011239078791094499, 0.010488261397857771, 0.010988705190382763, 0.010722130120087655, 0.011144396514464324, 0.010798407091927037, 0.010690288716879876, 0.010512306487799787, 0.010520316061796833, 0.01091179149343752, 0.0112592201324795, 0.010908567660476085, 0.01061053702495973, 0.010978781830166356, 0.010753296699716936, 0.010690492476866265, 0.01031256171042812, 0.010707423589314532, 0.010901555644908483, 0.010685948121177954, 0.010854955962580846, 0.01086711205092168, 0.01051026010587904, 0.011117771894433684, 0.010586215494474652, 0.011344277570355037, 0.011257870673236681, 0.010501640550215881, 0.010530510805542582, 0.010606392333213095, 0.010666138816590955, 0.01061390936216356, 0.01063874757034264, 0.010581216980824317, 0.010933417890057058, 0.010630411988316464, 0.011019332086205836, 0.01100767948444741, 0.01048797523205634, 0.010740512496586982, 0.010830256808358625, 0.010620501108287321, 0.010834630618586498, 0.010694813713273419, 0.010800527556494926, 0.010783656951861384, 0.010703193196179236, 0.010813684543174771, 0.010713574289814702, 0.011245911497486296, 0.011004597382025045, 0.010971252190312053, 0.01127796393821432, 0.010425734309040698, 0.010997739209447119, 0.010939088656295675, 0.010577647322070734, 0.010492271504748574, 0.010322723225399316, 0.010689696540218877, 0.010356147248697958, 0.010949629406460024, 0.01080368391701176, 0.010947743347716848, 0.010885605697523087, 0.011201217098109551, 0.010640956026770032, 0.011068040734601471, 0.010286833815268319, 0.010197265248151371, 0.010786265584417452, 0.010958191558612667, 0.01099888230352658, 0.010824869822211185, 0.010572784407707349, 0.010729641488206126, 0.010684506026403612, 0.010642807786209013, 0.010368587001713324, 0.010550394658291766, 0.010563197521972122, 0.010629594790711535, 0.010082612112231795, 0.010766305298926237, 0.01111778615462883, 0.010749798597341596, 0.010723068101352656, 0.010782014632996337, 0.010735500444887688, 0.010427856936034185, 0.010834348373612726, 0.01082131920594434, 0.010248709989160883, 0.010865276958233853, 0.010452664710397946, 0.010296125748789955, 0.010677958961090823, 0.010837491438225672, 0.01098783026833653, 0.010108517439347054, 0.010664022973137602, 0.010248851892616758, 0.010139488613143672, 0.010597743372907902, 0.010917172113950794, 0.010257960410066646, 0.010495832038196108, 0.01073022470717132, 0.010686545019614442, 0.010399178989775179, 0.010260293237184871, 0.010659145443713892, 0.0104389666572165, 0.010557089593715535, 0.010132879418490595, 0.010680086805167799, 0.010475516499868418, 0.009960992370068415, 0.010693398615187371, 0.010182917227297538, 0.010227340535020953, 0.01050723323469179, 0.010449469534173838, 0.010402543046371835, 0.010372737329461946, 0.010692736268461307, 0.010216442862863672, 0.010116935043892446, 0.010374318092467829, 0.010305896660537961, 0.010322399136579128, 0.010431262142370116, 0.01022831524686329, 0.010635370435839205, 0.010833459613342273, 0.010069709524269511, 0.010443504367721882, 0.010329239887706405, 0.011026586890533471, 0.01083031709997922, 0.01031454760062468, 0.009875897791733798, 0.01088147995149833, 0.010052610785805119, 0.010237245813524191, 0.010160141925266802, 0.009844826142764433, 0.009992038105531453, 0.010160586141231498, 0.01054929707943573, 0.010557565529103634, 0.010048810149853645, 0.010267895428193778, 0.010253265610614801, 0.010120976374268082, 0.01014566390745026, 0.010543481884075537, 0.01017340289901614, 0.009883646822097199, 0.010469090189408895, 0.010009962071851718, 0.010811406147348865, 0.010238268014406682, 0.009943285458112067, 0.010109916203983705, 0.010532195341594457, 0.009936167223690761, 0.009863402053637336, 0.010312397405209865, 0.010412520615923003, 0.010375773646985479, 0.010579422862107434, 0.01052347242386004, 0.01007006237802433, 0.010224899801559101, 0.010320434198473194, 0.010015607906901333, 0.010064049564034548, 0.010270894235992787, 0.010382171377508518, 0.01044660095841726, 0.010039204651033409, 0.010402132470212088],
        "eval_previous": [0.08896154165267944, 0.09419530630111694, 0.08126145601272583, 0.06088286638259888, 0.08187553286552429, 0.17918282747268677, 0.06116771697998047, 0.07829952239990234, 0.06558066606521606, 0.07000168412923813, 0.10218538343906403, 0.06895051151514053, 0.074799083173275, 0.0661376565694809, 0.06290815770626068, 0.05549833923578262, 0.07926825433969498, 0.048828691244125366, 0.057863205671310425, 0.057143405079841614, 0.05966991186141968, 0.057391561567783356, 0.05024910718202591, 0.07110579311847687, 0.06357263028621674, 0.055347539484500885, 0.051644716411828995, 0.05168180540204048, 0.05733927711844444, 0.05357825383543968, 0.045761577785015106, 0.047207482159137726, 0.05410505831241608, 0.06169970706105232, 0.05429694801568985, 0.04593948274850845, 0.05202121287584305, 0.04920237883925438, 0.04893288016319275, 0.04858493059873581, 0.051036253571510315, 0.05213205888867378, 0.05161762982606888, 0.05895065516233444, 0.05047230049967766, 0.05217670276761055, 0.05220368504524231, 0.051187675446271896, 0.05257386714220047, 0.04787846654653549, 0.03961271792650223, 0.05816147103905678, 0.05209854990243912, 0.04762998968362808, 0.04819308966398239, 0.04771817475557327, 0.05253436416387558, 0.04929102212190628, 0.05324907600879669, 0.0502055399119854, 0.054067373275756836, 0.04114088416099548, 0.04826591908931732, 0.0479629710316658, 0.04833730310201645, 0.06122761219739914, 0.04807593673467636, 0.05451729893684387, 0.04032459855079651, 0.047872886061668396, 0.044995248317718506, 0.04869566857814789, 0.04452921450138092, 0.043485015630722046, 0.049904271960258484, 0.041117049753665924, 0.04111367464065552, 0.04561472684144974, 0.05315365642309189, 0.050785839557647705, 0.052651599049568176, 0.04778595268726349, 0.04489140212535858, 0.047328490763902664, 0.04912508279085159, 0.05351122096180916, 0.03637281805276871, 0.047212496399879456, 0.04932805150747299, 0.05491504445672035, 0.04282993823289871, 0.05971226096153259, 0.04459305852651596, 0.045629631727933884, 0.04256581887602806, 0.04820258915424347, 0.04813282936811447, 0.042610324919223785, 0.05003880709409714, 0.04407951235771179, 0.04750942811369896, 0.045320067554712296, 0.040696680545806885, 0.045076269656419754, 0.04604499787092209, 0.04268178343772888, 0.04353155195713043, 0.04560569301247597, 0.04701612889766693, 0.04986010491847992, 0.042332470417022705, 0.03935414180159569, 0.045993395149707794, 0.051086682826280594, 0.04603815823793411, 0.05249318107962608, 0.04408474266529083, 0.04261007905006409, 0.04537983238697052, 0.04598647356033325, 0.0468723326921463, 0.046217262744903564, 0.044577643275260925, 0.04663648456335068, 0.050431277602910995, 0.047403376549482346, 0.04349733889102936, 0.044720664620399475, 0.04451676458120346, 0.04723319783806801, 0.04157969355583191, 0.050434693694114685, 0.04177762567996979, 0.04575473070144653, 0.043981343507766724, 0.0437651090323925, 0.046817291527986526, 0.04340876266360283, 0.05296029895544052, 0.047352470457553864, 0.04018896073102951, 0.04609721899032593, 0.04242170602083206, 0.049748700112104416, 0.04179946705698967, 0.045706748962402344, 0.04122991859912872, 0.04198087006807327, 0.052952930331230164, 0.04099979251623154, 0.04234663024544716, 0.03985188156366348, 0.038715604692697525, 0.04728791117668152, 0.041747085750103, 0.04779931902885437, 0.046037957072257996, 0.0455712154507637, 0.04505431652069092, 0.04054635763168335, 0.047175198793411255, 0.04347596317529678, 0.04233810678124428, 0.04740580543875694, 0.05034123733639717, 0.04406529292464256, 0.040314432233572006, 0.04915351793169975, 0.040445759892463684, 0.04589773342013359, 0.043541550636291504, 0.036892786622047424, 0.03893355652689934, 0.053451333194971085, 0.04549567401409149, 0.03659503906965256, 0.050924040377140045, 0.04190557077527046, 0.046279989182949066, 0.038649775087833405, 0.04524477198719978, 0.04128468409180641, 0.04226990416646004, 0.04429186135530472, 0.043692827224731445, 0.041242245584726334, 0.043460577726364136, 0.042360953986644745, 0.040447987616062164, 0.04079659283161163, 0.04642295837402344, 0.04165114462375641, 0.04143381863832474, 0.04509872570633888, 0.03782445937395096, 0.039202723652124405, 0.046096235513687134, 0.04174307361245155, 0.044243790209293365, 0.04140513390302658, 0.04081549495458603, 0.041428305208683014, 0.040965963155031204, 0.046912793070077896, 0.04004884883761406, 0.043315038084983826, 0.045848146080970764, 0.04025403782725334, 0.046781644225120544, 0.042532142251729965, 0.04490169882774353, 0.04308521747589111, 0.03975653648376465, 0.041676707565784454, 0.04922632873058319, 0.04458381235599518, 0.047789204865694046, 0.03946547210216522, 0.04545574635267258, 0.04208136349916458, 0.04411336034536362, 0.04408092051744461, 0.045432791113853455, 0.05148695409297943, 0.03995509073138237, 0.04217804968357086, 0.039269112050533295, 0.04049435257911682, 0.048310786485672, 0.041530296206474304, 0.04118015617132187, 0.046675294637680054, 0.044309988617897034, 0.04327966645359993, 0.0460432767868042, 0.04660215228796005, 0.044563136994838715, 0.04425323009490967, 0.041769951581954956, 0.038867801427841187, 0.045769572257995605, 0.0429680310189724, 0.04811543971300125, 0.04760152846574783, 0.043583061546087265, 0.04101439565420151, 0.0440547913312912, 0.03811419755220413, 0.04637840390205383, 0.04558907821774483, 0.03867284208536148, 0.04372512549161911, 0.04401957243680954, 0.04868168756365776, 0.04298334941267967, 0.04128287732601166, 0.04520038515329361, 0.04247283563017845, 0.043707020580768585, 0.03859855979681015, 0.04846159368753433, 0.04199041426181793, 0.04171368479728699, 0.04196006804704666, 0.046787962317466736, 0.04626674950122833, 0.04370211064815521, 0.041655778884887695, 0.0397985577583313, 0.04052708297967911, 0.05021185800433159, 0.04331351816654205, 0.047807443886995316, 0.048869211226701736, 0.037523381412029266, 0.04973951727151871, 0.03878895193338394, 0.04611533135175705, 0.04160439223051071, 0.04721645265817642, 0.041311219334602356, 0.04004734009504318, 0.043123893439769745, 0.04707680642604828, 0.04026040434837341, 0.040858194231987, 0.03784574568271637, 0.04760804772377014, 0.03990576043725014, 0.04477588087320328, 0.04339948296546936, 0.04427766427397728, 0.042006999254226685, 0.04384122043848038, 0.04171963781118393, 0.042432751506567, 0.04310578107833862, 0.040957603603601456, 0.041213586926460266, 0.04094601795077324, 0.04079898074269295, 0.040341489017009735, 0.04468052461743355, 0.03996897116303444, 0.04408358782529831, 0.039546899497509, 0.04735191911458969, 0.0378655381500721, 0.04445343837141991, 0.04479553550481796, 0.044837310910224915, 0.042007993906736374, 0.04145762324333191, 0.04588852822780609, 0.04320521652698517, 0.046517513692379, 0.04170721024274826, 0.042305782437324524, 0.04072750359773636, 0.045249395072460175, 0.04168889299035072]
    },

    "debug": { // arguments in debug mode, which will replace arguments in train
        "val_epoch": 1,
        "save_checkpoint_epoch": 1,
        "log_iter": 2,
        "debug_split": 50 // percent or number, change the size of dataloder to debug_split.
    }
}
